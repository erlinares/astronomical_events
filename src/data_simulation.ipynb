{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96006872-ff88-4884-a6db-d722ee7741bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Project: Lunar Events\n",
    "\n",
    "- Author:   Edgar Rios\n",
    "- Date:     2025-01-17\n",
    "- Version:  1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06262226-72f9-47db-a532-3b0deef24972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ******************************************************************************************************************\n",
    "# || DESCRIPTION\n",
    "# || -------------------------------------------------------------------------------------------------------------\n",
    "# || PROJECT       \t: Lunar Events\n",
    "# || FILE        \t: data_simulation.ipynb\n",
    "# || SOURCE         : \n",
    "# || TARGET         : /\n",
    "# || OBJETIVE\t\t: Create synthetic data of astronomical  events\n",
    "# || Reprocess      : Yes\n",
    "# || NOTES      \t: TBD\n",
    "# || SCHEDULER\t\t: TBD\n",
    "# || JOB\t\t\t: TBD\n",
    "# || VERSION  DEVELOPER\t        PROVIDER              DATE\t\t\t DESCRIPTION\n",
    "# || -------------------------------------------------------------------------------------------------------------\n",
    "# || \t1\t  EDGAR RIOS        SYNTHETIC       \t  2025-01-17\tCreate synthetic data of lunar events\n",
    "# || \t2\t  EDGAR RIOS        SYNTHETIC       \t  2025-01-19\tUpdate number of rows\n",
    "# ******************************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c54972-be18-4090-a887-0b2de44cc933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4154b4-9a20-4d35-8e42-2344c50f6440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894b69b6-cee9-4619-9638-140d94758d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2926997e-7220-4fd7-83ab-f1dd44e88e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a sample row\n",
    "def generate_row ():\n",
    "    event_types = [\"lunar\", \"solar\", \"asteroid\", \"meteor\", \"comet\"]\n",
    "    locations = [\"North_America\", \"South_America\", \"Europe\", \"Asia\", \"Africa\", \"Oceania\"]\n",
    "    \n",
    "    return {\n",
    "        \"event_id\": str(uuid.uuid4()),\n",
    "        \"event_type\": random.choice(event_types),\n",
    "        \"timestamp\": datetime.now() - timedelta(days=random.randint(0, 1095)),\n",
    "        \"location\": random.choice(locations),\n",
    "        \"details\": f\"Event description {random.randint(1000, 9999)}\"\n",
    "    }\n",
    "\n",
    "# create synthetic data\n",
    "def generate_dataset(spark, num_rows=1000000):\n",
    "    # Generamos datos en paralelo usando RDD\n",
    "    rdd = spark.sparkContext.parallelize(range(num_rows)) \\\n",
    "        .map(lambda x: generate_row())\n",
    "    \n",
    "    # convert to DataFrame\n",
    "    df = spark.createDataFrame(rdd)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "# create session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # synthetic data around  50GB\n",
    "    # by limits of databricks comunnity, we try with 5 GB\n",
    "    # Nota: num_rows for size of dataset 500000000\n",
    "    df = generate_dataset(spark, num_rows=500000)\n",
    "    \n",
    "    # save in parquet format\n",
    "    df.write.mode(\"overwrite\") \\\n",
    "        .partitionBy(\"timestamp\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .saveAsTable(\"default.astronomical_events\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_simulation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
